{
 "cells": [
  {
   "cell_type": "code",
   "id": "62b3a119",
   "metadata": {},
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "from browser import ChromeBrowser\n",
    "from browser.search.web import BraveBrowser\n",
    "from generalist.agents.core import CapabilityPlan\n",
    "from generalist.tools.data_model import ContentResource, ShortAnswer, Task\n",
    "\n",
    "\n",
    "class ExecutionState(TypedDict):\n",
    "    # what user is asking to do for them \n",
    "    ask: str\n",
    "    # Identifies what the original question/task given, which objective it got transferred to, what the plan to get an answer is\n",
    "    task: Task\n",
    "    # order index of the step of the task's plan that is being executed \n",
    "    step: int\n",
    "    # Clues, findings and answers to the previous subtasks\n",
    "    # Used to execute a capability plan step given already found information\n",
    "    context: str  \n",
    "    # capability plan for this task (overwritten when a new subtask from the main plain is picked up)\n",
    "    capability_plan: CapabilityPlan\n",
    "    # capability plan step order \n",
    "    capability_plan_step: int\n",
    "    # answers to subtask, the last one should be the final answer to the task \n",
    "    answers: list[ShortAnswer]\n",
    "    # all text resources that might be needed to execute the task\n",
    "    resources: list[ContentResource]\n",
    "    # tools that already got called\n",
    "    # TODO: see if this is needed \n",
    "    tools_called: str\n",
    "\n",
    "BRAVE_SEARCH = BraveBrowser(browser=ChromeBrowser(), session_id=\"deep_web_search\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "695079d0",
   "metadata": {},
   "source": [
    "from generalist.tools.text_processing import parse_resource\n",
    "import json\n",
    "from generalist.agents.core import AgentCapabilityDeepWebSearch, AgentCapabilityUnstructuredDataProcessor, AgentCapabilityCodeWritterExecutor, AgentCapabilityAudioProcessor\n",
    "from generalist.tools.planning import determine_capabilities, create_plan\n",
    "from generalist.tools.summarisers import construct_short_answer\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=ExecutionState)\n",
    "\n",
    "\n",
    "def init_state(ask: str, resources: list[ContentResource] = list()) -> ExecutionState:\n",
    "    # TODO: should I be using LLM to convert attachments/resources to acceptable format?\n",
    "    # TODO: implement proper handling of attachments and resources \n",
    "    return ExecutionState(\n",
    "        ask=ask,\n",
    "        task=None,\n",
    "        step=None,\n",
    "        context=\"\",\n",
    "        answers=list(),\n",
    "        resources=resources,\n",
    "    )\n",
    "\n",
    "def parse_ask(state: ExecutionState) -> ExecutionState:\n",
    "    \"\"\"\n",
    "    Check if there is additional information that needs to be converted into a resource in the task\n",
    "    \"\"\"\n",
    "    resource = parse_resource(state[\"ask\"])\n",
    "    if resource:\n",
    "        state[\"resources\"].append(resource)\n",
    "\n",
    "    return state\n",
    "\n",
    "def set_task(state: ExecutionState) -> ExecutionState:\n",
    "    question_task = state[\"ask\"]\n",
    "    resources = state[\"resources\"]\n",
    "    task_plan_response = create_plan(question_task, resources)\n",
    "\n",
    "    result = json.loads(task_plan_response)\n",
    "    task = Task(\n",
    "      question=question_task,\n",
    "      objective=result[\"objective\"],\n",
    "      plan=result[\"plan\"],\n",
    "    )\n",
    "    identified_resource = result.get(\"resource\", None)\n",
    "    if identified_resource:\n",
    "        task_resource = ContentResource(\n",
    "            provided_by=\"user\",\n",
    "            content=identified_resource.get(\"content\", None),\n",
    "            link=identified_resource.get(\"link\", None),\n",
    "            metadata={},\n",
    "        )\n",
    "        state[\"resources\"].append(task_resource)\n",
    "    state[\"task\"] = task\n",
    "\n",
    "    return state\n",
    "\n",
    "def set_plan_step(state: ExecutionState) -> ExecutionState: \n",
    "    \"\"\"  \n",
    "    Determine which state is being executed. \n",
    "    \"\"\"\n",
    "    if state[\"step\"] is None: \n",
    "        state[\"step\"] = 0 \n",
    "    else:\n",
    "        state[\"step\"] += 1\n",
    "         \n",
    "    return state\n",
    "\n",
    "def check_plan_completion(state: ExecutionState) -> str:\n",
    "    # FIXME: delete me, see what to do with the showrt_anser var, now only for debugging?\n",
    "    print(\"\\nSHORT ANSWER:\\n\", construct_short_answer(state[\"task\"].objective, state[\"context\"]), \"\\n\")\n",
    "    \n",
    "    # TODO: might incorporate early stopping if answer is found\n",
    "    # if not short_answer | len(short_answer.answer) == 0 | short_answer.answer.lower() != \"not found\":\n",
    "        \n",
    "    plan_length = len(state[\"task\"].plan)\n",
    "    if state[\"step\"] < plan_length:\n",
    "        return \"proceed\"\n",
    "    else:\n",
    "        return \"stop\"\n",
    "\n",
    "def set_capability_plan(state: ExecutionState) -> ExecutionState:\n",
    "    capability_plan_json = determine_capabilities(\n",
    "        state[\"task\"].plan[state[\"step\"]], \n",
    "        task=state[\"task\"], \n",
    "        resources=state[\"resources\"],\n",
    "        context=state[\"context\"]\n",
    "    )\n",
    "    state[\"capability_plan\"] = CapabilityPlan.from_json(capability_plan_json)\n",
    "    state[\"capability_plan_step\"] = 0\n",
    "\n",
    "    return state\n",
    "\n",
    "def set_capability_step(state: ExecutionState) -> ExecutionState:\n",
    "    if state[\"capability_plan_step\"] is None:\n",
    "        state[\"capability_plan_step\"] = 0 \n",
    "    else:\n",
    "        state[\"capability_plan_step\"] += 1 \n",
    "\n",
    "    return state\n",
    "\n",
    "def check_capability_step(state:ExecutionState) -> ExecutionState: \n",
    "    capability_plan_length = len(state[\"capability_plan\"].subplan)\n",
    "    if state[\"capability_plan_step\"] < capability_plan_length:\n",
    "        # run the next capability step\n",
    "        return \"iterate\"        \n",
    "    else: \n",
    "        # signal that we need to move over to the next state\n",
    "        return \"stop\" \n",
    "\n",
    "def run_capability(state: ExecutionState) -> ExecutionState: \n",
    "    activity, capability = state[\"capability_plan\"].subplan[state[\"capability_plan_step\"]]\n",
    "    output = None\n",
    "    capability_agent = capability(activity)\n",
    "\n",
    "    if capability is AgentCapabilityDeepWebSearch:\n",
    "        # Reinitiate the agent since it might need browser\n",
    "        capability_agent = capability(activity=activity, search_browser=BRAVE_SEARCH)\n",
    "        output = capability_agent.run()\n",
    "    elif capability is AgentCapabilityUnstructuredDataProcessor:\n",
    "        output = capability_agent.run(state[\"resources\"])\n",
    "    elif capability is AgentCapabilityCodeWritterExecutor:\n",
    "        output = capability_agent.run(state[\"resources\"])\n",
    "    elif capability is AgentCapabilityAudioProcessor:\n",
    "        output = capability_agent.run(state[\"resources\"])\n",
    "    else:\n",
    "        print(\"DEBUG | run_capability | Call to unidentified agent: \", capability)\n",
    "\n",
    "    answers = output.answers\n",
    "    resources = output.resources\n",
    "\n",
    "    if answers:\n",
    "        state[\"answers\"].extend(answers)\n",
    "    if resources:\n",
    "        state[\"resources\"].extend(resources)\n",
    "\n",
    "    return state\n",
    "\n",
    "def update_context(state: ExecutionState) -> ExecutionState:\n",
    "    asked = state[\"task\"].plan[state[\"step\"]]\n",
    "    found = state['answers']\n",
    "    context_delta = \"\\n\" + str(\n",
    "        {\n",
    "            \"asked\": asked,\n",
    "            \"found\": found,\n",
    "        }\n",
    "    ) \n",
    "    state[\"context\"] += context_delta\n",
    "\n",
    "    # TODO: IMPORTANT, do i need to save newly created resources somewhere else?\n",
    "    state[\"resources\"] = list()\n",
    "    return state\n",
    "\n",
    "FILE_NAME_SAVED_STATE = \"state.pkl\"\n",
    "def save_state(state: ExecutionState):\n",
    "    import pickle\n",
    "    \n",
    "    with open(FILE_NAME_SAVED_STATE, \"wb\") as f:\n",
    "        pickle.dump(state, f)\n",
    "\n",
    "def load_state(path: str = FILE_NAME_SAVED_STATE) -> ExecutionState:\n",
    "    import pickle\n",
    "\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Why: determine what we want to do and what we are given as input (e.g., some files)\n",
    "workflow.add_node(\"set_task\", set_task)\n",
    "# Check if there are any resources given in the task description\n",
    "workflow.add_node(\"parse_ask\", parse_ask)\n",
    "# Move over to producing an overarching plan: the steps of that plan you will follow one by one \n",
    "# Each step is a certain stage in getting to an answer.\n",
    "# E.g., plan = [Step1: searching internet for subquestion1; step2: search internet for an answer given answer to subquestion1 was YYY]\n",
    "workflow.add_node(\"set_plan_step\", set_plan_step)\n",
    "# Answer to each step is determined by create a capability plan, i.e., which specialised agent(s) we would like to involve to get an answer.\n",
    "# a capability plan is dependent on the context (i.e., all the previous finds that we collected from previous steps)\n",
    "workflow.add_node(\"set_capability_plan\", set_capability_plan)\n",
    "# Execute the capability plan one step at a time.\n",
    "# For example, 1)capability_plan_step1: search web ; 2)capability_plan_step2: analyse whatever you found in previously  \n",
    "workflow.add_node(\"run_capability\", run_capability)\n",
    "# Move over to the next capability_plan_step\n",
    "workflow.add_node(\"set_capability_step\", set_capability_step)\n",
    "# When the capability plan is finished - summarise the results into the context \n",
    "# Context = list of all the answers to capability_plan execution=plan_step 's  \n",
    "workflow.add_node(\"update_context\", update_context)\n",
    "\n",
    "workflow.add_edge(START,\"parse_ask\")\n",
    "workflow.add_edge(\"parse_ask\", \"set_task\")\n",
    "workflow.add_edge(\"set_task\", \"set_plan_step\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"set_plan_step\",\n",
    "    check_plan_completion,\n",
    "    {\n",
    "        \"proceed\": \"set_capability_plan\",\n",
    "        \"stop\": END,\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"set_capability_plan\", \"run_capability\")\n",
    "workflow.add_edge(\"run_capability\", \"set_capability_step\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"set_capability_step\", \n",
    "    check_capability_step,\n",
    "    {\n",
    "        \"iterate\":\"run_capability\",\n",
    "        \"stop\": \"update_context\",\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"update_context\", \"set_plan_step\")\n",
    "\n",
    "generalist_graph = workflow.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(generalist_graph.get_graph().draw_mermaid_png()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7fe976b8",
   "metadata": {},
   "source": [
    "# Test part nodes & logging\n",
    "import logging\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from generalist.models.core import MLFlowLLMWrapper\n",
    "from generalist.utils import pprint\n",
    "from generalist.tools import planning, web_search, text_processing, code\n",
    "\n",
    "# STARTING TO LOG EVERYTHING (MANUALLY ADDED)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment(\"test_generalist\")\n",
    "# MONKEY PATCH the llm call\n",
    "planning.llm = MLFlowLLMWrapper(planning.llm) \n",
    "web_search.llm = MLFlowLLMWrapper(web_search.llm)\n",
    "text_processing.llm = MLFlowLLMWrapper(text_processing.llm)\n",
    "code.llm = MLFlowLLMWrapper(code.llm)\n",
    "\n",
    "# FIXME: delete the if-statement, if wanting to test at this stage\n",
    "if False: \n",
    "    question = \"Who did the actor who played Ray in the Polish-language version of Everybody Loves Raymond play in Magda M.\"\n",
    "    initial_state = init_state(question)\n",
    "\n",
    "    state = set_task(initial_state)\n",
    "    pprint(state[\"task\"].__str__())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2c70f121",
   "metadata": {},
   "source": [
    "raise InterruptedError"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "47a60db1",
   "metadata": {},
   "source": [
    "# Test 1\n",
    "# MLFlow good at logging the state of the index and basic calls to LLM\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment(\"test_generalist\")\n",
    "mlflow.models.set_model(generalist_graph)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "question = \"Who did the actor who played Ray in the Polish-language version of Everybody Loves Raymond play in Magda M.\"\n",
    "\n",
    "initial_state = init_state(question)\n",
    "final_state = generalist_graph.invoke(initial_state)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dd37c0f9",
   "metadata": {},
   "source": [
    "# Task 2 \n",
    "# MLFlow good at logging the state of the index and basic calls to LLM\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment(\"test_generalist\")\n",
    "mlflow.models.set_model(generalist_graph)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "question = \"How many at bats did the Yankee with the most walks in the 1977 regular season have that same season?\"\n",
    "\n",
    "initial_state = init_state(question)\n",
    "final_state = generalist_graph.invoke(initial_state)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1926016b",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "# MLFlow good at logging the state of the index and basic calls to LLM\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment(\"test_generalist\")\n",
    "mlflow.models.set_model(generalist_graph)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "question = \"The attached Excel file contains the sales of menu items for a local fast-food chain. \" \\\n",
    "\"What were the total sales that the chain made from food (not including drinks)? Express your answer in USD with two decimal places.\"\n",
    "\n",
    "sources = [\n",
    "    ContentResource(\n",
    "        provided_by=\"user\", \n",
    "        content=\"\", \n",
    "        link=\"./7bd855d8-463d-4ed5-93ca-5fe35145f733.xls\",\n",
    "        metadata={},\n",
    "    )\n",
    "]\n",
    "initial_state = init_state(question, resources=sources)\n",
    "final_state = generalist_graph.invoke(initial_state)\n",
    "final_state"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b8eac40",
   "metadata": {},
   "source": [
    "# Task 4 : PDF \n",
    "# MLFlow good at logging the state of the index and basic calls to LLM\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment(\"test_generalist\")\n",
    "mlflow.models.set_model(generalist_graph)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# TODO: check if this task actually requires PDF processing \n",
    "question = \"Where were the Vietnamese specimens described by Kuznetzov in Nedoshivina's 2010 paper eventually deposited? \" \\\n",
    "\"Just give me the city name without abbreviations.\"\n",
    "\n",
    "sources = []\n",
    "initial_state = init_state(question, resources=sources)\n",
    "final_state = generalist_graph.invoke(initial_state)\n",
    "final_state"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b142e2ae",
   "metadata": {},
   "source": [
    "# Task 5: search and Information Retrieval from just text \n",
    "# MLFlow good at logging the state of the index and basic calls to LLM\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment(\"test_generalist\")\n",
    "mlflow.models.set_model(generalist_graph)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "question =\\\n",
    "\"\"\"What country had the least number of athletes at the 1928 Summer Olympics? If there's a tie for a number of athletes, return the first in alphabetical order. \n",
    "Give the IOC country code as your answer.\"\"\"\n",
    "\n",
    "initial_state = init_state(question)\n",
    "final_state = generalist_graph.invoke(initial_state)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f959622",
   "metadata": {},
   "source": [
    "# Task 6: python code execution  \n",
    "# MLFlow good at logging the state of the index and basic calls to LLM\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment(\"test_generalist\")\n",
    "mlflow.models.set_model(generalist_graph)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "question =\\\n",
    "\"\"\"What is the final numeric output from the attached Python code?\"\"\"\n",
    "\n",
    "sources = [\n",
    "    ContentResource(\n",
    "        provided_by=\"user\", \n",
    "        content=\"\", \n",
    "        link=\"/Users/maksim.rostov/pdev/freestyling/agents/hf-course/unit4_general_agent/generalist/notebooks/f918266a-b3e0-4914-865d-4faa564f1aef.py\",\n",
    "        metadata={},\n",
    "    )\n",
    "]\n",
    "initial_state = init_state(question, resources=sources)\n",
    "final_state = generalist_graph.invoke(initial_state)\n",
    "final_state"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f59fbd22",
   "metadata": {},
   "source": [
    "# Task 7: loading video file, transcribing it.  \n",
    "# MLFlow good at logging the state of the index and basic calls to LLM\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment(\"test_generalist\")\n",
    "mlflow.models.set_model(generalist_graph)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "question =\\\n",
    "\"\"\"Examine the video at https://www.youtube.com/watch?v=1htKBjuUWec. What does Teal'c say in response to the question \"Isn't that hot?\"\"\"\n",
    "\n",
    "initial_state = init_state(question)\n",
    "final_state = generalist_graph.invoke(initial_state)\n",
    "final_state"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
