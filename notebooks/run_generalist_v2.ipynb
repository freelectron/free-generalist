{
 "cells": [
  {
   "cell_type": "code",
   "id": "62b3a119",
   "metadata": {},
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "from browser import ChromeBrowser\n",
    "from browser.search.web import BraveBrowser\n",
    "from generalist.agents.core import CapabilityPlan\n",
    "from generalist.tools.data_model import ContentResource, ShortAnswer, Task\n",
    "\n",
    "\n",
    "class ExecutionState(TypedDict):\n",
    "    # what user is asking to do for them \n",
    "    ask: str\n",
    "    # Identifies what the original question/task given, which objective it got transferred to, what the plan to get an answer is\n",
    "    task: Task\n",
    "    # order index of the step of the task's plan that is being executed \n",
    "    step: int\n",
    "    # Clues, findings and answers to the previous subtasks\n",
    "    # Used to execute a capability plan step given already found information\n",
    "    context: str  \n",
    "    # capability plan for this task (overwritten when a new subtask from the main plain is picked up)\n",
    "    capability_plan: CapabilityPlan\n",
    "    # capability plan step order \n",
    "    capability_plan_step: int\n",
    "    # answers to subtask, the last one should be the final answer to the task \n",
    "    answers: list[ShortAnswer]\n",
    "    # all text resources that might be needed to execute the task\n",
    "    resources: list[ContentResource]\n",
    "    # tools that already got called\n",
    "    # TODO: see if this is needed \n",
    "    tools_called: str\n",
    "\n",
    "MAX_STEPS = 5\n",
    "BRAVE_SEARCH = BraveBrowser(browser=ChromeBrowser(), session_id=\"deep_web_search\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "695079d0",
   "metadata": {},
   "source": [
    "import json\n",
    "from generalist.agents.core import AgentCapabilityDeepWebSearch, AgentCapabilityUnstructuredDataProcessor, \\\n",
    "    AgentCapabilityCodeWriterExecutor, AgentCapabilityAudioProcessor, AgentCapabilityOutput\n",
    "from generalist.tools.planning import determine_capabilities, create_plan\n",
    "from generalist.tools.summarisers import construct_short_answer\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "def init_state(ask: str, resources: list[ContentResource] = list()) -> ExecutionState:\n",
    "    # TODO: should I be using LLM to convert attachments/resources to acceptable format?\n",
    "    # TODO: implement proper handling of attachments and resources \n",
    "    return ExecutionState(\n",
    "        ask=ask,\n",
    "        task=None,\n",
    "        step=None,\n",
    "        context=\"\",\n",
    "        answers=list(),\n",
    "        resources=resources,\n",
    "    )\n",
    "\n",
    "def set_task(state: ExecutionState) -> ExecutionState:\n",
    "    question_task = state[\"ask\"]\n",
    "    resources = state[\"resources\"]\n",
    "    task_plan_response = create_plan(question_task, resources)\n",
    "\n",
    "    result = json.loads(task_plan_response)\n",
    "    task = Task(\n",
    "      question=question_task,\n",
    "      objective=result[\"objective\"],\n",
    "      plan=result[\"plan\"],\n",
    "    )\n",
    "    identified_resource = result.get(\"resource\", None)\n",
    "    if identified_resource:\n",
    "        task_resource = ContentResource(\n",
    "            provided_by=\"user\",\n",
    "            content=identified_resource.get(\"content\", None),\n",
    "            link=identified_resource.get(\"link\", None),\n",
    "            metadata={},\n",
    "        )\n",
    "        state[\"resources\"].append(task_resource)\n",
    "    state[\"task\"] = task\n",
    "\n",
    "    state[\"step\"] = 0\n",
    "    return state\n",
    "\n",
    "def evaluate_task_completion(state: ExecutionState) -> str:\n",
    "    short_answer = construct_short_answer(\n",
    "        state[\"task\"].objective,\n",
    "        state[\"context\"]\n",
    "    )\n",
    "\n",
    "    # Early stopping if answer exists\n",
    "    if short_answer.answered:\n",
    "        return \"end\"\n",
    "\n",
    "    # Early stopping if maximum number of steps reached\n",
    "    if state['step'] > MAX_STEPS:\n",
    "        return \"end\"\n",
    "\n",
    "    return \"continue\"\n",
    "\n",
    "def plan_next_step(state: ExecutionState) -> ExecutionState:\n",
    "    # Automatically determine which step to execute based on context\n",
    "    capability_plan_json = determine_capabilities(\n",
    "        task=state[\"task\"],\n",
    "        context=state[\"context\"]\n",
    "    )\n",
    "\n",
    "    state[\"capability_plan\"] = CapabilityPlan.from_json(capability_plan_json)\n",
    "    return state\n",
    "\n",
    "def execute(state: ExecutionState) -> ExecutionState:\n",
    "    activity, capability = state[\"capability_plan\"].subplan[0]\n",
    "    output = AgentCapabilityOutput(activity)\n",
    "    capability_agent = capability(activity)\n",
    "\n",
    "    if capability is AgentCapabilityDeepWebSearch:\n",
    "        # Reinitiate the agent since it might need browser\n",
    "        capability_agent = capability(activity=activity, search_browser=BRAVE_SEARCH)\n",
    "        output = capability_agent.run()\n",
    "    elif capability is AgentCapabilityUnstructuredDataProcessor:\n",
    "        output = capability_agent.run(state[\"resources\"])\n",
    "    elif capability is AgentCapabilityCodeWriterExecutor:\n",
    "        output = capability_agent.run(state[\"resources\"])\n",
    "    elif capability is AgentCapabilityAudioProcessor:\n",
    "        output = capability_agent.run(state[\"resources\"])\n",
    "    else:\n",
    "        print(\"DEBUG | run_capability | Call to unidentified agent: \", capability)\n",
    "\n",
    "    if output.answers:\n",
    "        state[\"answers\"].extend(output.answers)\n",
    "    if output.resources:\n",
    "        state[\"resources\"].extend(output.resources)\n",
    "\n",
    "    # Update context with step results\n",
    "    state[\"context\"] += f\"\\nStep {state['step']}: {state['answers']}\"\n",
    "\n",
    "    # FIXME: after deep web search we need to pass the results to unstructered text processing\n",
    "    #  it can be a lot of text after deep web search and we probably want to analyse it once with (unstructered text processing)\n",
    "    #  and then clear out.\n",
    "    # Here I assume that unstructered text processing call comes directly after deep web search and we have finished analysing the results of deep web search\n",
    "    if capability is AgentCapabilityUnstructuredDataProcessor:\n",
    "        state[\"resources\"] = [ r for r in state[\"resources\"]  if r.provided_by != AgentCapabilityDeepWebSearch.name ]\n",
    "\n",
    "\n",
    "    state[\"step\"] += 1\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=ExecutionState)\n",
    "\n",
    "workflow.add_node(\"set_task\", set_task)\n",
    "workflow.add_node(\"plan_next_step\", plan_next_step)\n",
    "workflow.add_node(\"execute\", execute)\n",
    "\n",
    "workflow.add_edge(START, \"set_task\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"set_task\",\n",
    "    evaluate_task_completion,\n",
    "    {\n",
    "        \"continue\": \"plan_next_step\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"plan_next_step\", \"execute\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"execute\",\n",
    "    evaluate_task_completion,\n",
    "    {\n",
    "        \"continue\": \"plan_next_step\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "\n",
    "generalist_graph = workflow.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(generalist_graph.get_graph().draw_mermaid_png()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7fe976b8",
   "metadata": {},
   "source": [
    "# Test part nodes & logging\n",
    "import logging\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from generalist.models.core import MLFlowLLMWrapper\n",
    "from generalist.utils import pprint\n",
    "from generalist.tools import planning, web_search, text_processing, code\n",
    "\n",
    "# STARTING TO LOG EVERYTHING (MANUALLY ADDED)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment(\"test_generalist\")\n",
    "# MONKEY PATCH the llm call\n",
    "planning.llm = MLFlowLLMWrapper(planning.llm) \n",
    "web_search.llm = MLFlowLLMWrapper(web_search.llm)\n",
    "text_processing.llm = MLFlowLLMWrapper(text_processing.llm)\n",
    "code.llm = MLFlowLLMWrapper(code.llm)\n",
    "\n",
    "# FIXME: delete the if-statement, if wanting to test at this stage\n",
    "if False: \n",
    "    question = \"Who did the actor who played Ray in the Polish-language version of Everybody Loves Raymond play in Magda M.\"\n",
    "    initial_state = init_state(question)\n",
    "\n",
    "    state = set_task(initial_state)\n",
    "    pprint(state[\"task\"].__str__())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2c70f121",
   "metadata": {},
   "source": [
    "raise InterruptedError"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "47a60db1",
   "metadata": {},
   "source": [
    "# Test 1\n",
    "# MLFlow good at logging the state of the index and basic calls to LLM\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment(\"test_generalist\")\n",
    "mlflow.models.set_model(generalist_graph)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "question = \"Who did the actor who played Ray in the Polish-language version of Everybody Loves Raymond play in Magda M.\"\n",
    "\n",
    "initial_state = init_state(question)\n",
    "final_state = generalist_graph.invoke(initial_state)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dd37c0f9",
   "metadata": {},
   "source": [
    "# Task 2 \n",
    "# MLFlow good at logging the state of the index and basic calls to LLM\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment(\"test_generalist\")\n",
    "mlflow.models.set_model(generalist_graph)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "question = \"How many at bats did the Yankee with the most walks in the 1977 regular season have that same season?\"\n",
    "\n",
    "initial_state = init_state(question)\n",
    "final_state = generalist_graph.invoke(initial_state)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1926016b",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "# MLFlow good at logging the state of the index and basic calls to LLM\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment(\"test_generalist\")\n",
    "mlflow.models.set_model(generalist_graph)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "question = \"The attached Excel file contains the sales of menu items for a local fast-food chain. \" \\\n",
    "\"What were the total sales that the chain made from food (not including drinks)? Express your answer in USD with two decimal places.\"\n",
    "\n",
    "sources = [\n",
    "    ContentResource(\n",
    "        provided_by=\"user\", \n",
    "        content=\"\", \n",
    "        link=\"./7bd855d8-463d-4ed5-93ca-5fe35145f733.xls\",\n",
    "        metadata={},\n",
    "    )\n",
    "]\n",
    "initial_state = init_state(question, resources=sources)\n",
    "final_state = generalist_graph.invoke(initial_state)\n",
    "final_state"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b8eac40",
   "metadata": {},
   "source": [
    "# Task 4 : PDF \n",
    "# MLFlow good at logging the state of the index and basic calls to LLM\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment(\"test_generalist\")\n",
    "mlflow.models.set_model(generalist_graph)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# TODO: check if this task actually requires PDF processing \n",
    "question = \"Where were the Vietnamese specimens described by Kuznetzov in Nedoshivina's 2010 paper eventually deposited? \" \\\n",
    "\"Just give me the city name without abbreviations.\"\n",
    "\n",
    "sources = []\n",
    "initial_state = init_state(question, resources=sources)\n",
    "final_state = generalist_graph.invoke(initial_state)\n",
    "final_state"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b142e2ae",
   "metadata": {},
   "source": [
    "# Task 5: search and Information Retrieval from just text \n",
    "# MLFlow good at logging the state of the index and basic calls to LLM\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment(\"test_generalist\")\n",
    "mlflow.models.set_model(generalist_graph)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "question =\\\n",
    "\"\"\"What country had the least number of athletes at the 1928 Summer Olympics? If there's a tie for a number of athletes, return the first in alphabetical order. \n",
    "Give the IOC country code as your answer.\"\"\"\n",
    "\n",
    "initial_state = init_state(question)\n",
    "final_state = generalist_graph.invoke(initial_state)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f959622",
   "metadata": {},
   "source": [
    "# Task 6: python code execution  \n",
    "# MLFlow good at logging the state of the index and basic calls to LLM\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment(\"test_generalist\")\n",
    "mlflow.models.set_model(generalist_graph)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "question =\\\n",
    "\"\"\"What is the final numeric output from the attached Python code?\"\"\"\n",
    "\n",
    "sources = [\n",
    "    ContentResource(\n",
    "        provided_by=\"user\", \n",
    "        content=\"\", \n",
    "        link=\"/Users/maksim.rostov/pdev/freestyling/agents/hf-course/unit4_general_agent/generalist/notebooks/f918266a-b3e0-4914-865d-4faa564f1aef.py\",\n",
    "        metadata={},\n",
    "    )\n",
    "]\n",
    "initial_state = init_state(question, resources=sources)\n",
    "final_state = generalist_graph.invoke(initial_state)\n",
    "final_state"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f59fbd22",
   "metadata": {},
   "source": [
    "# Task 7: loading video file, transcribing it.  \n",
    "# MLFlow good at logging the state of the index and basic calls to LLM\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment(\"test_generalist\")\n",
    "mlflow.models.set_model(generalist_graph)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "question =\\\n",
    "\"\"\"Examine the video at https://www.youtube.com/watch?v=1htKBjuUWec. What does Teal'c say in response to the question \"Isn't that hot?\"\"\"\n",
    "\n",
    "initial_state = init_state(question)\n",
    "final_state = generalist_graph.invoke(initial_state)\n",
    "final_state"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Try evaluation with GAIA\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "gaia_path = \"~/pdev/freelectron/free-generalist/evaluation/gaia\"\n",
    "data_dir = snapshot_download(local_dir=gaia_path,  local_files_only=True, repo_id=\"gaia-benchmark/GAIA\", repo_type=\"dataset\")\n",
    "\n",
    "dataset = load_dataset(data_dir, \"2023_level1\", split=\"validation\")\n",
    "gaia_keys = ['task_id', 'Question', 'Level', 'Final answer', 'file_name', 'file_path', 'Annotator Metadata']\n",
    "for sample in dataset:\n",
    "    [ print(k, \"=\", sample[k]) for k in gaia_keys]\n",
    "\n",
    "    mlflow.langchain.autolog()\n",
    "    mlflow.set_tracking_uri('http://localhost:5000')\n",
    "    mlflow.set_experiment(f\"gaia_{\"_\".join(sample[\"task_id\"].split(\"-\"))}\")\n",
    "    mlflow.models.set_model(generalist_graph)\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    question = sample[\"Question\"]\n",
    "    initial_state = init_state(question)\n",
    "    final_state = generalist_graph.invoke(initial_state)\n",
    "    break"
   ],
   "id": "559a61b6c053932a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
