{
 "cells": [
  {
   "cell_type": "code",
   "id": "62b3a119",
   "metadata": {},
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "from browser import ChromeBrowser\n",
    "from browser.search.web import BraveBrowser\n",
    "from generalist.agents.core import CapabilityPlan\n",
    "from generalist.tools.data_model import ContentResource, ShortAnswer, Task\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class ExecutionState(TypedDict):\n",
    "    # what user is asking to do for them \n",
    "    ask: str\n",
    "    # Identifies what the original question/task given, which objective it got transferred to, what the plan to get an answer is\n",
    "    task: Task\n",
    "    # order index of the step of the task's plan that is being executed \n",
    "    step: int\n",
    "    # Clues, findings and answers to the previous subtasks\n",
    "    # Used to execute a capability plan step given already found information\n",
    "    context: str  \n",
    "    # capability plan for this task (overwritten when a new subtask from the main plain is picked up)\n",
    "    capability_plan: CapabilityPlan\n",
    "    # capability plan step order \n",
    "    capability_plan_step: int\n",
    "    # answers to subtask, the last one should be the final answer to the task \n",
    "    answers: list[ShortAnswer]\n",
    "    # all text resources that might be needed to execute the task\n",
    "    resources: list[ContentResource]\n",
    "    # tools that already got called\n",
    "    # TODO: see if this is needed \n",
    "    tools_called: str\n",
    "\n",
    "MAX_STEPS = 2\n",
    "BRAVE_SEARCH = BraveBrowser(browser=ChromeBrowser(), session_id=\"deep_web_search\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "695079d0",
   "metadata": {},
   "source": [
    "import json\n",
    "from generalist.agents.core import AgentCapabilityDeepWebSearch, AgentCapabilityUnstructuredDataProcessor, \\\n",
    "    AgentCapabilityCodeWriterExecutor, AgentCapabilityAudioProcessor, AgentCapabilityOutput\n",
    "from generalist.tools.planning import determine_capabilities, create_plan\n",
    "from generalist.tools.summarisers import construct_short_answer\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "def init_state(ask: str, resources: list[ContentResource] | None = None) -> ExecutionState:\n",
    "    return ExecutionState(\n",
    "        ask=ask,\n",
    "        task=None,\n",
    "        step=None,\n",
    "        context=\"\",\n",
    "        answers=list(),\n",
    "        resources=resources if resources else [],\n",
    "    )\n",
    "\n",
    "def set_task(state: ExecutionState) -> ExecutionState:\n",
    "    question_task = state[\"ask\"]\n",
    "    resources = state[\"resources\"]\n",
    "    task_plan_response = create_plan(question_task, resources)\n",
    "\n",
    "    result = json.loads(task_plan_response)\n",
    "    task = Task(\n",
    "      question=question_task,\n",
    "      objective=result[\"objective\"],\n",
    "      plan=result[\"plan\"],\n",
    "    )\n",
    "    identified_resource = result.get(\"resource\", None)\n",
    "    if identified_resource:\n",
    "        task_resource = ContentResource(\n",
    "            provided_by=\"user\",\n",
    "            content=identified_resource.get(\"content\", None),\n",
    "            link=identified_resource.get(\"link\", None),\n",
    "            metadata={},\n",
    "        )\n",
    "        state[\"resources\"].append(task_resource)\n",
    "    state[\"task\"] = task\n",
    "\n",
    "    state[\"step\"] = 0\n",
    "    return state\n",
    "\n",
    "def evaluate_task_completion(state: ExecutionState) -> str:\n",
    "    short_answer = construct_short_answer(\n",
    "        state[\"task\"].objective,\n",
    "        state[\"context\"]\n",
    "    )\n",
    "\n",
    "    # Early stopping if answer exists\n",
    "    if short_answer.answered:\n",
    "        return \"end\"\n",
    "\n",
    "    # Early stopping if maximum number of steps reached\n",
    "    if state['step'] >= MAX_STEPS:\n",
    "        return \"end\"\n",
    "\n",
    "    return \"continue\"\n",
    "\n",
    "def plan_next_step(state: ExecutionState) -> ExecutionState:\n",
    "    # Automatically determine which step to execute based on context\n",
    "    capability_plan_json = determine_capabilities(\n",
    "        task=state[\"task\"],\n",
    "        context=state[\"context\"]\n",
    "    )\n",
    "\n",
    "    state[\"capability_plan\"] = CapabilityPlan.from_json(capability_plan_json)\n",
    "    return state\n",
    "\n",
    "def execute(state: ExecutionState) -> ExecutionState:\n",
    "    activity, capability = state[\"capability_plan\"].subplan[0]\n",
    "    output = AgentCapabilityOutput(activity)\n",
    "    capability_agent = capability(activity)\n",
    "\n",
    "    if capability is AgentCapabilityDeepWebSearch:\n",
    "        # Reinitiate the agent since it might need browser\n",
    "        capability_agent = capability(activity=activity, search_browser=BRAVE_SEARCH)\n",
    "        output = capability_agent.run()\n",
    "    elif capability is AgentCapabilityUnstructuredDataProcessor:\n",
    "        output = capability_agent.run(state[\"resources\"])\n",
    "    elif capability is AgentCapabilityCodeWriterExecutor:\n",
    "        output = capability_agent.run(state[\"resources\"])\n",
    "    elif capability is AgentCapabilityAudioProcessor:\n",
    "        output = capability_agent.run(state[\"resources\"])\n",
    "    else:\n",
    "        print(\"DEBUG | run_capability | Call to unidentified agent: \", capability)\n",
    "\n",
    "    if output.answers:\n",
    "        state[\"answers\"].extend(output.answers)\n",
    "    if output.resources:\n",
    "        state[\"resources\"].extend(output.resources)\n",
    "\n",
    "    # Update context with step results\n",
    "    state[\"context\"] += f\"\\nStep {state['step']}: {state['answers']}\"\n",
    "    state[\"step\"] += 1\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=ExecutionState)\n",
    "\n",
    "workflow.add_node(\"set_task\", set_task)\n",
    "workflow.add_node(\"plan_next_step\", plan_next_step)\n",
    "workflow.add_node(\"execute\", execute)\n",
    "\n",
    "workflow.add_edge(START, \"set_task\")\n",
    "workflow.add_edge(\"set_task\", \"plan_next_step\")\n",
    "workflow.add_edge(\"plan_next_step\", \"execute\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"execute\",\n",
    "    evaluate_task_completion,\n",
    "    {\n",
    "        \"continue\": \"plan_next_step\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "\n",
    "generalist_graph = workflow.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(generalist_graph.get_graph().draw_mermaid_png()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7fe976b8",
   "metadata": {},
   "source": [
    "from generalist.models.core import MLFlowLLMWrapper\n",
    "from generalist.tools import planning, web_search, text_processing, code\n",
    "\n",
    "# MONKEY PATCH THE LLM CALLS TO HAVE THE LOGGING IN STDOUT\n",
    "planning.llm = MLFlowLLMWrapper(planning.llm) \n",
    "web_search.llm = MLFlowLLMWrapper(web_search.llm)\n",
    "text_processing.llm = MLFlowLLMWrapper(text_processing.llm)\n",
    "code.llm = MLFlowLLMWrapper(code.llm)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import mlflow\n",
    "from huggingface_hub import snapshot_download\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "gaia_path = os.environ.get(\"HUGGING_FACE_GAIA_FOLDER_PATH\")\n",
    "data_dir = snapshot_download(local_dir=gaia_path,  local_files_only=True, repo_id=\"gaia-benchmark/GAIA\", repo_type=\"dataset\")\n",
    "\n",
    "dataset = load_dataset(data_dir, \"2023_level1\", split=\"validation\")\n",
    "gaia_keys = ['task_id', 'Question', 'Level', 'Final answer', 'file_name', 'file_path', 'Annotator Metadata']\n",
    "\n",
    "sosa_many_studio_albums_task_id = \"8e867cd7-cff9-4e6c-867a-ff5ddc2550be\"\n",
    "running_to_the_moon_task_id = \"e1fc63a2-da7a-432f-be78-7c4a95598703\"\n",
    "dr_who_season_9_eps_11_location_task_id = \"4b6bb5f7-f634-410e-815d-e673ab7f8632\"\n",
    "calc_sales_xlsx_task_id = \"7bd855d8-463d-4ed5-93ca-5fe35145f733\"\n",
    "just_running_python_task_id = \"f918266a-b3e0-4914-865d-4faa564f1aef\"\n",
    "evaluation_tasks = [\n",
    "    just_running_python_task_id,\n",
    "    calc_sales_xlsx_task_id,\n",
    "    # sosa_many_studio_albums_task_id,\n",
    "    # running_to_the_moon_task_id,\n",
    "]\n",
    "\n",
    "results = []\n",
    "dataset_questions = { sample[\"task_id\"]:sample for sample in dataset }\n",
    "for sample_task_id in evaluation_tasks:\n",
    "    sample = dataset_questions[sample_task_id]\n",
    "    [ print(k, \"=\", sample[k]) for k in gaia_keys]\n",
    "\n",
    "    mlflow.langchain.autolog()                                                     # this is needed to register traces within the experiment\n",
    "    experiment_name = f\"gaia_{\"_\".join(sample[\"task_id\"].split(\"-\"))}\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    mlflow.models.set_model(generalist_graph)\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    experiment_url = mlflow.get_experiment_by_name(experiment_name)\n",
    "    # mlflow.set_tracking_uri('http://localhost:5000')\n",
    "\n",
    "    question = sample[\"Question\"]\n",
    "    resources = []\n",
    "    if sample[\"file_path\"]:\n",
    "        resource = ContentResource(\n",
    "            provided_by=\"user\",\n",
    "            content=\"file provided with the main task\",\n",
    "            link=os.path.join(os.environ.get(\"HUGGING_FACE_GAIA_FOLDER_PATH\"), sample[\"file_path\"]),\n",
    "            metadata={\"note\":\"the file is already in the list of available resources\"}\n",
    "        )\n",
    "        print(resource.link)\n",
    "        resources.append(resource)\n",
    "    initial_state = init_state(question, resources=resources)\n",
    "    final_state = generalist_graph.invoke(initial_state)\n",
    "    answers = final_state[\"answers\"]\n",
    "    results.append((sample, {\"answers\":answers, \"experiment_url\": experiment_url}))"
   ],
   "id": "ba4ee28d950c37fb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
